{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "https://www.geeksforgeeks.org/cifar-10-image-classification-in-tensorflow/\n",
    "https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/\n",
    "https://www.33rdsquare.com/convolutional-neural-network-pytorch-implementation-on-cifar10-dataset/\n",
    "https://www.geeksforgeeks.org/building-a-convolutional-neural-network-using-pytorch/\n",
    "https://ncl.instructure.com/courses/55046/files/8928967?module_item_id=3535263\n",
    "https://ncl.instructure.com/courses/55046/files/8913068?module_item_id=3535259\n",
    "https://ncl.instructure.com/courses/55046/files/8942227?module_item_id=3540677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\" \n",
    "        My neural network model for CIFAR-10 classification created using different sources listed in the References section.\n",
    "        @param batch_normalization: Whether to use batch normalization or not. It is used to answer Q2 \n",
    "        \n",
    "      \"\"\"\n",
    "    def __init__(self, batch_normalization=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.batch_normalization = batch_normalization\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU()]\n",
    "            if batch_normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            return layers\n",
    "\n",
    "        self.block1 = nn.Sequential(*conv_block(3, 32), *conv_block(32, 32), nn.MaxPool2d(2, 2))\n",
    "        self.block2 = nn.Sequential(*conv_block(32, 64), *conv_block(64, 64), nn.MaxPool2d(2, 2))\n",
    "        self.block3 = nn.Sequential(*conv_block(64, 128), *conv_block(128, 128), nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\" \n",
    "        Load CIFAR-10 dataset and return train and test loaders.\n",
    "      \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\" \n",
    "        Count the total number of parameters in the model. It is used to keep track of the complexity of the model.\n",
    "      \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def train_model(model, trainloader, testloader, epochs=50, device='cuda', early_stopping_patience=10):\n",
    "    \"\"\" \n",
    "        Train the model using the given train and test loaders. It is used to train the model and return the best model. \n",
    "\n",
    "        @param model: The model to train\n",
    "        @param trainloader: The DataLoader for the training set\n",
    "        @param testloader: The DataLoader for the test set\n",
    "        @param epochs: The number of epochs to train the model, default is 50\n",
    "        @param device: The device to use for training, default is 'cuda'\n",
    "        @param early_stopping_patience: The number of epochs to wait before early stopping, default is 10\n",
    "\n",
    "        @info : loss function is CrossEntropyLoss and optimizer is Adam\n",
    "      \"\"\"\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model = model.to(device)\n",
    "    train_losses, test_losses = [], []\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fun(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_loss /= len(testloader)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            patience = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, test_losses, model\n",
    "\n",
    "def plot_convergence(train_losses, test_losses, title, filename):\n",
    "    \"\"\" \n",
    "        Plot the convergence of the model during training. It is used to visualize the training process and compare my models.\n",
    "      \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_filters(model, test_loader, device):\n",
    "    \"\"\" \n",
    "        Plot the feature filters for each layer in the model. Used to visualize the feature filters of each convolutional layer. \n",
    "        We can see the dimensions of the feature maps and how they change as we go deeper into the network.\n",
    "      \"\"\"\n",
    "    model.eval()\n",
    "    test_images, _ = next(iter(test_loader))\n",
    "    test_image = test_images[0].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        activations = []\n",
    "        x = test_image\n",
    "        for layer in list(model.block1) + list(model.block2) + list(model.block3):\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                activations.append(x.cpu().numpy())\n",
    "\n",
    "    for i, activation in enumerate(activations):\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        num_filters = activation.shape[1]\n",
    "        for j in range(min(num_filters, 16)):\n",
    "            plt.subplot(4, 4, j + 1)\n",
    "            plt.imshow(activation[0, j, :, :], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f\"Layer {i + 1} Feature Maps\")\n",
    "        plt.show()\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\" \n",
    "        Run the experiment to train the model with and without batch normalization. This is the main function to run. \n",
    "        It will get the data, train the models with different layers. It will plot the convergence and feature filters for the models. \n",
    "        \n",
    "      \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    trainloader, testloader = load_dataset()\n",
    "\n",
    "    print(\"Training without batch normalization...\")\n",
    "    model_without_bn = NeuralNet(batch_normalization=False)\n",
    "    print(f\"Total parameters: {count_parameters(model_without_bn)}\")\n",
    "    train_losses_no_bn, test_losses_no_bn, best_model_no_bn = train_model(\n",
    "        model_without_bn, trainloader, testloader, epochs=50, device=device\n",
    "    )\n",
    "    torch.save(best_model_no_bn.state_dict(), 'model_without_bn.pth')\n",
    "\n",
    "    print(\"Training with batch normalization...\")\n",
    "    model_with_bn = NeuralNet(batch_normalization=True)\n",
    "    print(f\"Total parameters: {count_parameters(model_with_bn)}\")\n",
    "    train_losses_with_bn, test_losses_with_bn, best_model_with_bn = train_model(\n",
    "        model_with_bn, trainloader, testloader, epochs=50, device=device\n",
    "    )\n",
    "    torch.save(best_model_with_bn.state_dict(), 'model_with_bn.pth')\n",
    "\n",
    "    # Plot convergence\n",
    "    plot_convergence(\n",
    "        train_losses_no_bn, test_losses_no_bn,\n",
    "        \"Model Convergence Without Batch Normalization\", \"convergence_no_bn.png\"\n",
    "    )\n",
    "    plot_convergence(\n",
    "        train_losses_with_bn, test_losses_with_bn,\n",
    "        \"Model Convergence With Batch Normalization\", \"convergence_with_bn.png\"\n",
    "    )\n",
    "\n",
    "    # Plot feature filters\n",
    "    print(\"Plotting feature filters from the best model with batch normalization...\")\n",
    "    plot_feature_filters(model_with_bn, testloader, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
