{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWPbX4HvOh4-"
   },
   "source": [
    "# Predictive Analytics, Computer Vision & AI - CSC3831\n",
    "## Coursework, Part 2: Machine Learning\n",
    "\n",
    "As this coursework is as much about practical skills as it is about reflecting on the procedures and the results, you are expected to explain what you did, your reasoning for process decisions, as well as a thorough analysis of your results.\n",
    "\n",
    "### 1. Load the MNIST dataset, visualise the first 20 digits, and print their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qaf0EfQswKbZ"
   },
   "outputs": [],
   "source": [
    "# Run this to load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier,\n",
    "    KNeighborsRegressor,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,\n",
    "    plot_tree,\n",
    ")\n",
    "\n",
    "train_img, train_labels = fetch_openml(\n",
    "    'mnist_784', version=1, return_X_y=True, parser='auto', as_frame=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(4, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(train_img[i].reshape(28, 28), cmap='gray')\n",
    "    axi.set(xticks=[], yticks=[], xlabel=train_labels[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qusqC8Zf5vQN"
   },
   "source": [
    "### 2. Train a Logistic Regression classifier on this data, and report on your findings.\n",
    "    \n",
    "1. Tune your hyperparameters to ensure *sparse* weight vectors and high accuracy.\n",
    "2. Visualise the classification vector for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "train_img, test_img, train_labels, test_labels = train_test_split(\n",
    "    train_img, train_labels,\n",
    "    test_size=1/7.0,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scM7z69-524T"
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "train_img_scaled = scaler.fit_transform(train_img)\n",
    "test_img_scaled = scaler.transform(test_img)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1],  \n",
    "    'penalty': ['l2'],  \n",
    "    'max_iter': [10,100, 1000]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=SEED,solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "grid_search.fit(train_img_scaled, train_labels)\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = best_model.predict(test_img_scaled)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(\"\\nTest accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "print(\"score : \" + str(best_model.score(test_img_scaled, test_labels)))\n",
    "\n",
    "\n",
    "# Visualize classification vectors\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    weights = best_model.coef_[i].reshape(28, 28)\n",
    "    plt.imshow(weights, cmap='seismic', interpolation='nearest')\n",
    "    plt.title(f'Digit {i}')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "# user a scaler : \n",
    "# https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_to_Speed-up_Machine_Learning_Algorithms.ipynb \n",
    "\n",
    "# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPnwMGuk531k"
   },
   "source": [
    "### 3. Use PCA to reduce the dimensionality of your training data.\n",
    "    \n",
    "1. Determine the number of components necessary to explain 80\\% of the variance\n",
    "2. Plot the explained variance by number of components.\n",
    "3. Visualise the 20 principal components' loadings\n",
    "4. Plot the two principal components for your data using a scatterplot, colouring by class. What can you say about this plot?\n",
    "5. Visualise the first 20 digits, *generated from their lower-dimensional representation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://builtin.com/machine-learning/pca-in-python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# take the minimum of components such as 95% of the variance is retained\n",
    "pca = PCA(0.95)\n",
    "\n",
    "pca.fit(train_img_scaled)\n",
    "print(\"Number of components:\", pca.n_components_)\n",
    "\n",
    "train_img_pca = pca.transform(train_img_scaled)\n",
    "test_img_pca = pca.transform(test_img_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRyxcSS_6Czn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# nb_comp = np.where(np.cumsum(pca.explained_variance_ratio_) > 0.8)[0][0]\n",
    "# print(\"Number of components for 80% variance:\", nb_comp)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.axvline(x=nb_comp, color='r', linestyle='--')\n",
    "# plt.axhline(y=0.8, color='g', linestyle='--')\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Cumulative explained variance')\n",
    "# plt.savefig('pca_variance_explained.png')\n",
    "# plt.show()\n",
    "\n",
    "# Plot the first 20 principal components' loadings\n",
    "# fig, ax = plt.subplots(4, 5, figsize=(15, 8))\n",
    "# for i, axi in enumerate(ax.flat):\n",
    "#     component = pca.components_[i].reshape(28, 28)\n",
    "#     vmax = np.abs(component).max()\n",
    "#     axi.imshow(component, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "#     axi.set_title(f'PC {i+1}')\n",
    "#     axi.axis('off')\n",
    "# plt.suptitle('First 20 Principal Components Loadings', y=1.02, size=14)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('pca_loadings.png')\n",
    "# plt.show()\n",
    "# answer : The plot shows the loadings of the first 20 principal components. Each principal component is represented as a 28x28 image, where each pixel corresponds to a feature in the original dataset. The color of each pixel indicates the weight of the corresponding feature in the principal component. Blue pixels represent negative weights, red pixels represent positive weights, and white pixels represent zero weights. The intensity of the color indicates the magnitude of the weight. By examining the loadings of the principal components, we can gain insights into the underlying structure of the data and identify patterns that are important for explaining the variance in the dataset.\n",
    "\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=train_img_pca[:, 0], y=train_img_pca[:, 1], hue=train_labels, palette='tab10')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2D Scatter Plot of Principal Components')\n",
    "plt.legend(title='Classes')\n",
    "plt.savefig('pca_scatter_classes_visu.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJnvCd7a6D1k"
   },
   "source": [
    "### 4. Generate a noisy copy of your data by adding random normal noise to the digits **with a scale that doesn't completely destroy the signal**. This is, the resulting images noise should be apparent, but the numbers should still be understandable.\n",
    "        \n",
    "1. Visualise the first 20 digits from the noisy dataset.\n",
    "2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n",
    "3. Visualise the first 20 digits of the de-noised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc6A12yH66Dp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def analyze_variance_thresholds(noisy_train_img, thresholds=[0.7, 0.8, 0.85, 0.9, 0.95, 0.99]):\n",
    "    n_features = noisy_train_img.shape[1]\n",
    "    scaler = StandardScaler()\n",
    "    noisy_train_img = scaler.fit_transform(noisy_train_img)\n",
    "    pca = PCA()\n",
    "    pca.fit(noisy_train_img)\n",
    "    \n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "             pca.explained_variance_ratio_, 'bo-', markersize=2)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Scree Plot: Individual Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(1, len(cumsum) + 1), cumsum, 'r-')\n",
    "    \n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(thresholds)))\n",
    "    for thr, color in zip(thresholds, colors):\n",
    "        n_components = np.argmax(cumsum >= thr) + 1\n",
    "        plt.axhline(y=thr, color=color, linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=n_components, color=color, linestyle='--', alpha=0.5)\n",
    "        plt.text(n_components + 10, thr, \n",
    "                f'{thr:.0%} var: {n_components} components',\n",
    "                color=color)\n",
    "    \n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_variance_thresholds.png')\n",
    "    \n",
    "    print(\"\\nDetailed Analysis of PCA Components:\")\n",
    "    print(\"-\" * 50)\n",
    "    for thr in thresholds:\n",
    "        n_components = np.argmax(cumsum >= thr) + 1\n",
    "        compression_ratio = (n_features - n_components) / n_features * 100\n",
    "        print(f\"\\nAt {thr:.0%} explained variance:\")\n",
    "        print(f\"- Number of components needed: {n_components}\")\n",
    "        print(f\"- Dimension reduction: {n_features} → {n_components}\")\n",
    "        print(f\"- Compression ratio: {compression_ratio:.1f}%\")\n",
    "    \n",
    "    return pca\n",
    "\n",
    "def visualize_reconstruction_quality(train_img, noisy_train_img, variance_thresholds=[0.8, 0.9, 0.95]):\n",
    "    fig, axes = plt.subplots(len(variance_thresholds) + 2, 5, \n",
    "                            figsize=(15, 3*(len(variance_thresholds) + 2)))\n",
    "    \n",
    "    for i in range(5):\n",
    "        axes[0, i].imshow(train_img[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title('Original' if i == 0 else '')\n",
    "        \n",
    "        axes[1, i].imshow(noisy_train_img[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title('Noisy' if i == 0 else '')\n",
    "    \n",
    "    for idx, threshold in enumerate(variance_thresholds):\n",
    "        pca = PCA(n_components=np.argmax(PCA().fit(noisy_train_img).explained_variance_ratio_.cumsum() >= threshold) + 1)\n",
    "        reconstructed = pca.inverse_transform(pca.fit_transform(noisy_train_img))\n",
    "        \n",
    "        for i in range(5):\n",
    "            axes[idx + 2, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "            axes[idx + 2, i].axis('off')\n",
    "            axes[idx + 2, i].set_title(f'{threshold:.0%} var' if i == 0 else '')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_reconstruction_quality.png')\n",
    "    plt.show()\n",
    "\n",
    "def determine_optimal_variance(train_img, noisy_train_img):\n",
    "    pca = analyze_variance_thresholds(noisy_train_img)\n",
    "\n",
    "    visualize_reconstruction_quality(train_img, noisy_train_img)\n",
    "    \n",
    "    return pca\n",
    "\n",
    "\n",
    "\n",
    "def add_noise(X, noise_level=0.5):\n",
    "    return X + noise_level * np.random.randn(*X.shape)\n",
    "\n",
    "pca = determine_optimal_variance(train_img, add_noise(train_img))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd01hlKbk0I0pS7qPYqWVL",
   "provenance": [
    {
     "file_id": "1wYzbvkrKUbmJfJFl7wQdkb2GFDtG9NW3",
     "timestamp": 1730027292420
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
