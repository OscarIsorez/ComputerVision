{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWPbX4HvOh4-"
   },
   "source": [
    "# Predictive Analytics, Computer Vision & AI - CSC3831\n",
    "## Coursework, Part 2: Machine Learning\n",
    "\n",
    "As this coursework is as much about practical skills as it is about reflecting on the procedures and the results, you are expected to explain what you did, your reasoning for process decisions, as well as a thorough analysis of your results.\n",
    "\n",
    "### 1. Load the MNIST dataset, visualise the first 20 digits, and print their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qaf0EfQswKbZ"
   },
   "outputs": [],
   "source": [
    "# Run this to load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier,\n",
    "    KNeighborsRegressor,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,\n",
    "    plot_tree,\n",
    ")\n",
    "\n",
    "train_img, train_labels = fetch_openml(\n",
    "    'mnist_784', version=1, return_X_y=True, parser='auto', as_frame=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(4, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(train_img[i].reshape(28, 28), cmap='gray')\n",
    "    axi.set(xticks=[], yticks=[], xlabel=train_labels[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results \n",
    "The MNIST dataset is a very old dataset that contains 70,000 images of handwritten digits from 0 to 9. It is a reference dataset for machine learning and computer vision. We can visualise the first 20 digits of the dataset and print their corresponding labels. We can use these labels to use a supervised learning algorithm to predict the labels of the rest of the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qusqC8Zf5vQN"
   },
   "source": [
    "### 2. Train a Logistic Regression classifier on this data, and report on your findings.\n",
    "    \n",
    "1. Tune your hyperparameters to ensure *sparse* weight vectors and high accuracy.\n",
    "2. Visualise the classification vector for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "train_img, test_img, train_labels, test_labels = train_test_split(\n",
    "    train_img, train_labels,\n",
    "    test_size=1/7.0,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seperate the dataset into training and testing sets. I did 1/7 split for the training and testing sets because the dataset is 70000 images large. I used the SEED 42 to ensure reproducibility. This way we'll be able to fit our model on the training set and evaluate it on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# we fit on training set only.\n",
    "train_img_scaled = scaler.fit_transform(train_img)\n",
    "test_img_scaled = scaler.transform(test_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I standardised the data using the StandardScaler from the sklearn library. This is important because the Logistic Regression algorithm is sensitive to the scale of the features. I fit the training set only because for instance we don't want to introduce any bias into the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scM7z69-524T"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1],  \n",
    "    'penalty': ['l2'],  \n",
    "    'max_iter': [10,100, 1000]\n",
    "}\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=SEED,solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "grid_search.fit(train_img_scaled, train_labels)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "\n",
    "y_pred = best_model.predict(test_img_scaled)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(\"\\nTest accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "print(\"score : \" + str(best_model.score(test_img_scaled, test_labels)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "##### Context\n",
    "First things first, this code takes a long time to run (several hours). I had to stop the code because it was taking too long to run. This has been the main issue with this part of the coursework. I made some personal research on Logistic Regression and found out that it is not the best algorithm for image classification. I the Biomedical Analytics and AI course we use deep learning methods to perform on this dataset. It was a really interesting exercise too. \n",
    "I used \"lbfgs\" solver because apparently the default solver is insanly slow. Because we perform hyperparameter tuning, we need to optimize the parameters we are using to actually get a result. \n",
    "\n",
    "##### Interpretation \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_classification_vectors(model):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        weights = model.coef_[i].reshape(28, 28)\n",
    "        plt.imshow(weights, cmap='seismic', interpolation='nearest')\n",
    "        plt.title(f'Digit {i}')\n",
    "        plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_classification_vectors(best_model)\n",
    "\n",
    "\n",
    "\n",
    "# user a scaler: \n",
    "# https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_to_Speed-up_Machine_Learning_Algorithms.ipynb \n",
    "\n",
    "#  Documentation and resources on the topic:\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPnwMGuk531k"
   },
   "source": [
    "### 3. Use PCA to reduce the dimensionality of your training data.\n",
    "    \n",
    "1. Determine the number of components necessary to explain 80\\% of the variance\n",
    "2. Plot the explained variance by number of components.\n",
    "3. Visualise the 20 principal components' loadings\n",
    "4. Plot the two principal components for your data using a scatterplot, colouring by class. What can you say about this plot?\n",
    "5. Visualise the first 20 digits, *generated from their lower-dimensional representation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://builtin.com/machine-learning/pca-in-python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# take the minimum of components such as 95% of the variance is retained\n",
    "pca = PCA(0.95)\n",
    "\n",
    "pca.fit(train_img)\n",
    "print(\"Original scale:\", train_img.shape)\n",
    "print(\"Number of components:\", pca.n_components_)\n",
    "\n",
    "train_img_pca = pca.transform(train_img)\n",
    "test_img_pca = pca.transform(test_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "We come from a 784-dimensional space to a 330-dimensional space. I choose 0.95 as the threshold because it is a common threshold to use with PCA. We can already that that we need 330 components to explain 95% of the variance but for the purpose of this exercise we will use 80% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRyxcSS_6Czn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_cum_variance_explained(pca):\n",
    "    nb_comp = np.where(np.cumsum(pca.explained_variance_ratio_) > 0.8)[0][0]\n",
    "    print(\"Number of components for 80% variance:\", nb_comp)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.axvline(x=nb_comp, color='r', linestyle='--')\n",
    "    plt.axhline(y=0.8, color='g', linestyle='--')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.savefig('pca_variance_explained.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_var_over_comp(pca):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(pca.explained_variance_ratio_, marker='o')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance Ratio by Principal Component')\n",
    "    plt.grid()\n",
    "    # plt.savefig('pca_variance_ratio.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "plot_cum_variance_explained(pca)\n",
    "plot_var_over_comp(pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "We can see on the first plot that we need 147 components to explain 80% of the variance. PCA did a good job of dimentional reduction on this dataset. On the second plot we can see the curve of the variance explained for each principal components. I understand why the author of the exercise choosed 80% as the threshold. We can see that the curve is almost linear after 147 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loadings(pca):\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(15, 8))\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        component = pca.components_[i].reshape(28, 28)\n",
    "        vmax = np.abs(component).max()\n",
    "        axi.imshow(component, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "        axi.set_title(f'PC {i+1}')\n",
    "        axi.axis('off')\n",
    "    plt.suptitle('First 20 Principal Components Loadings', y=1.02, size=14)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('pca_loadings.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_loadings(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The plot  shows the loadings of the first 20 principal components. Each principal component is represented as \n",
    "a 28x28 image, where each pixel corresponds to a feature in the original dataset. The color of each pixel indicates\n",
    " the weight of the corresponding feature in the principal component. Blue pixels represent negative weights, red pixels \n",
    "represent positive weights, and white pixels represent zero weights. The intensity of the color indicates the magnitude \n",
    "of the weight. By examining the loadings of the principal components, we can gain insights into the underlying structure\n",
    " of the data and identify patterns that are important for explaining the variance in the dataset.\n",
    "\n",
    "Source of help: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_P1_over_P2(train_img_pca):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=train_img_pca[:, 0], y=train_img_pca[:, 1], hue=train_labels, palette='tab10')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('2D Scatter Plot of Principal Components')\n",
    "    plt.legend(title='Classes')\n",
    "    # plt.savefig('pca_scatter_classes_visu.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_P1_over_P2( train_img_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation \n",
    "\n",
    "TODO with claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_digits_from_pca(pca, data):\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(15, 8))\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        digit = pca.inverse_transform(data[i]).reshape(28, 28)\n",
    "        axi.imshow(digit, cmap='gray')\n",
    "        axi.set(xticks=[], yticks=[])\n",
    "    plt.suptitle('First 20 Digits from PCA', y=1.02, size=14)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('pca_digits.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_digits_from_pca(pca, train_img_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results \n",
    "\n",
    "Using the inverse_transform feature of PCA allowed use to get pretty good results. We can see that the digits are not as clear as the original images but we can still see the digits. What we could do in extension would be to train a model on the PCA transformed data and see how it performs, make a comparison with the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJnvCd7a6D1k"
   },
   "source": [
    "### 4. Generate a noisy copy of your data by adding random normal noise to the digits **with a scale that doesn't completely destroy the signal**. This is, the resulting images noise should be apparent, but the numbers should still be understandable.\n",
    "        \n",
    "1. Visualise the first 20 digits from the noisy dataset.\n",
    "2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n",
    "3. Visualise the first 20 digits of the de-noised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc6A12yH66Dp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def add_noise(data, noise_level=0.1):\n",
    "    return data + noise_level * np.random.normal(loc=0, scale=1, size=data.shape)\n",
    "\n",
    "noised_train_img = add_noise(train_img, noise_level=0.4)\n",
    "noised_test_img = add_noise(test_img, noise_level=0.4)\n",
    "\n",
    "noised_train_img = np.clip(noised_train_img, 0, 1)\n",
    "noised_test_img = np.clip(noised_test_img, 0, 1)\n",
    "\n",
    "def plot_20_digits(data, title):\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(15, 8))\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        axi.imshow(data[i].reshape(28, 28), cmap='gray')\n",
    "        axi.set(xticks=[], yticks=[])\n",
    "    plt.suptitle(title, y=1.02, size=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_20_digits(train_img, 'Original scaled digits')\n",
    "plot_20_digits(noised_train_img, 'Noised digits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "I've added random noise from normal distribution with mean = 0 and std =1\n",
    "\n",
    "https://youtu.be/Sm54KXD-L1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = [0.7, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_features = pca.n_features_in_\n",
    "\n",
    "# for thr in thresholds:\n",
    "#         n_components = np.argmax(cumsum >= thr) + 1\n",
    "#         compression_ratio = (n_features - n_components) / n_features * 100\n",
    "#         print(f\"\\nAt {thr:.0%} explained variance:\")\n",
    "#         print(f\"- Number of components needed: {n_components}\")\n",
    "#         print(f\"- Dimension reduction: {n_features} → {n_components}\")\n",
    "#         print(f\"- Compression ratio: {compression_ratio:.1f}%\")\n",
    "\n",
    "# plot the variance explained by the components\n",
    "\n",
    "pca = PCA(0.95)\n",
    "pca.fit(noised_train_img)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "sufficient_variance = 0.80\n",
    "pca = PCA(sufficient_variance)\n",
    "pca.fit(noised_train_img)\n",
    "noised_train_img_pca = pca.transform(noised_train_img)\n",
    "noised_test_img_pca = pca.transform(noised_test_img)\n",
    "\n",
    "denoised_train_img = pca.inverse_transform(noised_train_img_pca)\n",
    "\n",
    "\n",
    "plot_20_digits(noised_train_img, 'Noised digits')\n",
    "plot_20_digits(denoised_train_img, 'Denoised digits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def add_noise(data, noise_level=0.1):\n",
    "    return data + noise_level * np.random.normal(loc=0, scale=1, size=data.shape)\n",
    "\n",
    "# Plot function\n",
    "def plot_20_digits(data, title):\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(15, 8))\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        axi.imshow(data[i].reshape(28, 28), cmap='gray')\n",
    "        axi.set(xticks=[], yticks=[])\n",
    "    plt.suptitle(title, y=1.02, size=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "noise_level = 0.3\n",
    "noised_train_img = add_noise(train_img, noise_level=noise_level)\n",
    "noised_test_img = add_noise(test_img, noise_level=noise_level)\n",
    "\n",
    "noised_train_img = np.clip(noised_train_img, 0, 1)\n",
    "noised_test_img = np.clip(noised_test_img, 0, 1)\n",
    "\n",
    "# PCA with cumulative explained variance\n",
    "pca = PCA()\n",
    "pca.fit(noised_train_img)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "sufficient_variance = 0.95\n",
    "\n",
    "\n",
    "plt.plot(cumsum, label='Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs Components')\n",
    "plt.axhline(y=sufficient_variance, color='r', linestyle='--', label=f'{sufficient_variance}% Variance Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(sufficient_variance)\n",
    "pca.fit(noised_train_img)\n",
    "\n",
    "noised_train_img_pca = pca.transform(noised_train_img)\n",
    "denoised_train_img = pca.inverse_transform(noised_train_img_pca)\n",
    "denoised_test_img = pca.inverse_transform(pca.transform(noised_test_img))\n",
    "\n",
    "plot_20_digits(train_img, 'Original Digits')\n",
    "plot_20_digits(noised_train_img, 'Noisy Digits')\n",
    "plot_20_digits(denoised_train_img, 'Denoised Digits')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "\n",
    "rf_classifier.fit(train_img, train_labels)\n",
    "y_pred_original = rf_classifier.predict(test_img)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "y_pred_noised = rf_classifier.predict(noised_test_img)\n",
    "accuracy_noised = accuracy_score(test_labels, y_pred_noised)\n",
    "\n",
    "y_pred_denoised = rf_classifier.predict(denoised_test_img)\n",
    "accuracy_denoised = accuracy_score(test_labels, y_pred_denoised)\n",
    "\n",
    "\n",
    "print(f\"Original test accuracy: {accuracy:.4f}\")\n",
    "print(f\"Noised test accuracy: {accuracy_noised:.4f}\")\n",
    "print(f\"Denoised test accuracy: {accuracy_denoised:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd01hlKbk0I0pS7qPYqWVL",
   "provenance": [
    {
     "file_id": "1wYzbvkrKUbmJfJFl7wQdkb2GFDtG9NW3",
     "timestamp": 1730027292420
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
