{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsFtldtMYMGG"
   },
   "source": [
    "# CSC3831 Final Assessment - Part I: Data Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CXEwmOVfYG8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "houses_corrupted = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/CORRUPTED/HOUSES/houses_0.1_MAR.csv', header=0)\n",
    "\n",
    "# for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UkViOchMMIg"
   },
   "source": [
    "Above we've loaded in a corrupted version of a housing dataset. The anomalies need to be dealt with and missing values imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abwbd_vBYsv7"
   },
   "source": [
    "### 1. Data Understanding [7]\n",
    "- Perform ad hoc EDA to understand and describe what you see in the raw dataset\n",
    "  - Include graphs, statistics, and written descritpions as appropriate\n",
    "  - Any extra information about the data you can provide here is useful, think about performing an analysis (ED**A**), what would you find interesting or useful?\n",
    "- Identify features with missing records, outlier records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis is the process of using querying and visualization techniques to examine the surface properties of acquired data. This includes:\n",
    "\n",
    "- Simple statistical analysis\n",
    "- Distribution of attributes\n",
    "- Relationships between pairs or small numbers of attributes\n",
    "\n",
    "First, we are going to check the data itself, to see the shape of the data and the first rows. Then, we are going to use some methods to print tables of information and statistics about the data. We are also going to plot some graphs to visualize the data and make some assumptions about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G04uriMrZH7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(houses_corrupted.head())\n",
    "\n",
    "\n",
    "# print(houses_corrupted.info())\n",
    "print()\n",
    "print(houses_corrupted.describe(include='all'))\n",
    "\n",
    "print(houses_corrupted.info())\n",
    "\n",
    "print(houses_corrupted.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msno.matrix(houses_corrupted)\n",
    "houses_corrupted.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### 1. Data Understanding\n",
    "We can see that the Unnamed: 0 column is not necessary, so we are going to drop it. We can also see that there are some missing values in the data in the median_outcome, housing_median_age and population column. \n",
    "We can see that all column are numerical, which is going to help later on because dealing with categorical data can include \n",
    "additional steps. We have some information about the distribution as well, especially the quartiles, the mean and the standard deviation.\n",
    "\n",
    "### 2. Plots\n",
    "\n",
    "We can see visualy the missing data in 3 columns, we can see the distribution of the data in the columns, and we can see the correlation between the columns.\n",
    "Moreover, some outliers are detected in the data, which we are going to deal with in the next steps, especially because : \n",
    "\n",
    "> Outliers can skew a dataset and influence our measure of centre\n",
    "> and spread. Depending on skew use the metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these findings, we will then proceed to clean the remove the useless data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    houses_corrupted.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"already dropped\")\n",
    "\n",
    "sns.histplot(houses_corrupted['median_house_value'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no string features so we can skip the encoding part, and we don't have to worry about the categorical data. We will only focus on the numerical data. We wont need to look for malformed strings too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to make a pairplot to look for any relationship between the features. We are going to use the seaborn library to make the pairplot. It is also going to be useful to describe the distribution of the data, the skewness, and the outliers.\n",
    "\n",
    "> Def:\n",
    "> Pairs plots are a series of\n",
    "> scatter plots of every attribute\n",
    "> present in the data.\n",
    "> â€¢ Useful for an initial look at\n",
    "> relationships in the data\n",
    "> â€¢ Visually overwhelming for\n",
    "> final reports\n",
    "> â€¢ Utilise to detect interesting\n",
    "> relationships to analyse further\n",
    "\n",
    "1_Introduction_to_Data_Science.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_corrupted)\n",
    "print(houses_corrupted.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results :\n",
    "\n",
    "We can see that there is a strong correlation between the total_bedroom and housholds. Some other features are also strongly corrolated with each other as we can see on this pairplot. \n",
    "\n",
    "In the diagonal, we can see the distribution of the data. We can see that the data is not normally distributed, and we can see that there are some outliers in the data. We can see that the data is positively skewed,like the figure 2.1 of 1_Introduction_to_Data_Science.pdf show. This is going to be useful to know for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Imputation [10]\n",
    "- Identify which features should be imputed and which should be removed\n",
    "  - Provide a written rationale for this decision\n",
    "- Impute the missing records using KNN imputation\n",
    "- Impute the missing records using MICE imputation\n",
    "- Compare both imputed datasets feature distributions against each other and the non-imputed data\n",
    "- Build a regressor on all thre datasets\n",
    "  - Use regression models to predict house median price\n",
    "  - Compare regressors of non-imputed data against imputed datas\n",
    "  - **Note**: If you're struggling to compare against the original dataset focus on comparing the two imputed datasets against each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "# Identify features to impute or remove\n",
    "missing_threshold = 0.3\n",
    "features_to_impute = []\n",
    "features_to_remove = []\n",
    "\n",
    "for column in houses_corrupted.columns:\n",
    "    missing_percentage = houses_corrupted[column].isnull().sum() / houses_corrupted.shape[0]\n",
    "    if missing_percentage > missing_threshold:\n",
    "        features_to_remove.append(column)\n",
    "    elif missing_percentage > 0:\n",
    "        features_to_impute.append(column)\n",
    "\n",
    "print(\"Features to impute:\", features_to_impute)\n",
    "print(\"Features to remove:\", features_to_remove)\n",
    "\n",
    "houses_corrupted = houses_corrupted.drop(columns=features_to_remove)\n",
    "\n",
    "X = houses_corrupted.drop(columns=['median_house_value'])\n",
    "y = houses_corrupted['median_house_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_knn_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_test_knn_imputed = knn_imputer.transform(X_test)\n",
    "\n",
    "mice_imputer = IterativeImputer(random_state=SEED)\n",
    "X_train_mice_imputed = mice_imputer.fit_transform(X_train)\n",
    "X_test_mice_imputed = mice_imputer.transform(X_test)\n",
    "\n",
    "def plot_feature_distributions(original, knn_imputed, mice_imputed, feature_names):\n",
    "    fig, axes = plt.subplots(len(feature_names), 3, figsize=(15, len(feature_names) * 5))\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        sns.histplot(original[feature], ax=axes[i, 0], kde=True, color='blue')\n",
    "        axes[i, 0].set_title(f'Original {feature}')\n",
    "        sns.histplot(knn_imputed[:, i], ax=axes[i, 1], kde=True, color='green')\n",
    "        axes[i, 1].set_title(f'KNN Imputed {feature}')\n",
    "        sns.histplot(mice_imputed[:, i], ax=axes[i, 2], kde=True, color='red')\n",
    "        axes[i, 2].set_title(f'MICE Imputed {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distributions(X_train, X_train_knn_imputed, X_train_mice_imputed, features_to_impute)\n",
    "\n",
    "def build_and_evaluate_regressor(X_train, X_test, y_train, y_test):\n",
    "    # regressor = DecisionTreeRegressor(random_state=SEED)\n",
    "    # regressor = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, r2\n",
    "\n",
    "X_train_original = X_train.dropna()\n",
    "y_train_original = y_train[X_train_original.index]\n",
    "X_test_original = X_test.dropna()\n",
    "y_test_original = y_test[X_test_original.index]\n",
    "\n",
    "mse_original, r2_original = build_and_evaluate_regressor(X_train_original, X_test_original, y_train_original, y_test_original)\n",
    "print(f'Original Data - MSE: {mse_original}, R2: {r2_original}')\n",
    "\n",
    "# KNN Imputed data\n",
    "mse_knn, r2_knn = build_and_evaluate_regressor(X_train_knn_imputed, X_test_knn_imputed, y_train, y_test)\n",
    "print(f'KNN Imputed Data - MSE: {mse_knn}, R2: {r2_knn}')\n",
    "\n",
    "# MICE Imputed data\n",
    "mse_mice, r2_mice = build_and_evaluate_regressor(X_train_mice_imputed, X_test_mice_imputed, y_train, y_test)\n",
    "print(f'MICE Imputed Data - MSE: {mse_mice}, R2: {r2_mice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Interpretation\n",
    "We can see that the KNN got a very similar distribution to the original data, more that MICE. \n",
    "\n",
    "### Imputation\n",
    "\n",
    "KNN/MICE. The two imputer got a pretty similar result, which are quite good, compared to the original data.\n",
    "\n",
    "### Regression\n",
    "I tried to use DecisionTree, RandomForest and LinearRegression to predict the median_house_value. They got pretty similar results, so I kept to LinearRegressor because as we discussed in class, it allows use to keep the intepretation without the complexity of the other models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR74DAF_ZQUy"
   },
   "source": [
    "### 2. Outlier Identification [10]\n",
    "- Utilise a statistical outlier detection approach (i.e., **no** KNN, LOF, 1Class SVM)\n",
    "- Utilise an algorithmic outlier detection method of your choice\n",
    "- Compare results and decide what to do with identified outleirs\n",
    "  - Include graphs, statistics, and written descriptions as appropriate\n",
    "- Explain what you are doing, and why your analysis is appropriate\n",
    "- Comment on benefits/detriments of statistical and algorithmic outlier detection approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers in a dataset are either:\n",
    "- Rare: Appear with low frequency relative to the rest of the data (inliers)\n",
    "- Unusual: Do not fit the data distribution\n",
    "\n",
    "\n",
    "Missing at Random (MAR) is data where there may be a systemic\n",
    "reason why some of the data is missing, but this knowledge does\n",
    "not help us with imputation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing I think that dealing the missing values in this section is relevant. In fact we can see that there are some missing values in the data, and we are going to use the KNN imputer to fill the missing values. We are going to use the KNN imputer because it is a good imputer for numerical data, and it is going to take into account the correlation between the features. Moreover, We need data without missing values to detect the outliers, so I feel like it is a good idea to fill the missing values before detecting the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dataset for comparison against the imputed datasets\n",
    "houses = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/ORIGINAL/houses.csv', header=0)\n",
    "\n",
    "original_columns = houses_corrupted.columns\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "houses_corrupted = pd.DataFrame(knn_imputer.fit_transform(houses_corrupted), columns=original_columns)\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print( houses_corrupted.isnull().sum() )\n",
    "\n",
    "houses_corrupted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADs and Z-scores\n",
    "\n",
    "We are now going to use the robust Z-score as discussed in the lecture because the data is skewed and include outliers\n",
    "\n",
    "> MADs Def: \n",
    "> Median of all absolute\n",
    "> deviations from the median\n",
    "> ð‘€ð´ð· = 1.483 âˆ— ð‘šð‘’ð‘‘ð‘–:1â€¦ð‘›(|ð‘¥ð‘– âˆ’ ð‘šð‘’ð‘‘(ð‘¥ð‘—)ð‘—:1â€¦ð‘›|)\n",
    "\n",
    "> Z-score Def:\n",
    "> Z-score is a conversion of standard deviation from a normal distribution to a\n",
    "> standard normal distribution.\n",
    "\n",
    "\n",
    "ð‘‹ = ð‘¥1, â€¦ , ð‘¥ð‘›\n",
    "ð‘Ÿð‘œð‘_ð‘§ð‘– =\n",
    "ð‘¥ð‘– âˆ’ ð‘šð‘’ð‘‘(ð‘¥)\n",
    "ð‘€ð´ð·\n",
    "\n",
    "\n",
    "I thought multiple time to scale the data using the standard scaler, but I decided not to do it and work with the raw data. In fact, scaling skewed data is not very effective. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPsaKYCZZPkv"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "columns_to_analyze = ['population', 'median_income', 'housing_median_age']\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    # ð‘‹ = {ð‘¥1, â€¦ , ð‘¥ð‘›}  ð‘Ÿð‘œð‘_ð‘§ð‘– = (ð‘¥ð‘– âˆ’ ð‘šð‘’ð‘‘(ð‘¥) )/ ð‘€ð´ð·\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    robust_z_scores = 0.6745 * (data - median) / mad\n",
    "    return np.abs(robust_z_scores) > threshold\n",
    "\n",
    "def detect_outliers_isolation_forest(data):\n",
    "    isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    outliers = isolation_forest.fit_predict(data)\n",
    "    return outliers == -1\n",
    "\n",
    "def compare_outliers(data, columns):\n",
    "    fig, axes = plt.subplots(len(columns), 2, figsize=(15, len(columns) * 5))\n",
    "    for i, column in enumerate(columns):\n",
    "        # DÃ©tection des valeurs aberrantes\n",
    "        outliers_zscore = detect_outliers_zscore(data[column])\n",
    "        outliers_iforest = detect_outliers_isolation_forest(data[[column]])\n",
    "        \n",
    "        sns.histplot(data[column], ax=axes[i, 0], kde=True, color='blue')\n",
    "        sns.histplot(data[column][outliers_zscore], ax=axes[i, 0], kde=True, color='red')\n",
    "        axes[i, 0].set_title(f'{column} - Z-score')\n",
    "        axes[i, 0].text(0.05, 0.95, 'Blue: Original\\nRed: Outliers', transform=axes[i, 0].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))\n",
    "        \n",
    "        sns.histplot(data[column], ax=axes[i, 1], kde=True, color='blue')\n",
    "        sns.histplot(data[column][outliers_iforest], ax=axes[i, 1], kde=True, color='red')\n",
    "        axes[i, 1].set_title(f'{column} - Isolation Forest')\n",
    "        axes[i, 1].text(0.05, 0.95, 'Blue: Original\\nRed: Outliers', transform=axes[i, 1].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_outliers(houses_corrupted, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : \n",
    "###  Chart analysis : \n",
    "\n",
    "Population :\n",
    "\n",
    "Z-score: Outliers are mainly located on the right of the distribution (very high values).\n",
    "Isolation Forest: Outliers are similar but slightly more numerous, with slightly wider detection.\n",
    "Median Income :\n",
    "\n",
    "Z-score: Only very high values are detected as outliers.\n",
    "Isolation Forest: Detection is more diverse, with anomalies in the high and low parts, indicating that it considers very low and very high incomes to be abnormal.\n",
    "\n",
    "Housing Median Age :\n",
    "Z-score: Detects few anomalies, only at the right end (high values).\n",
    "Isolation Forest: More diversity in anomalies, with better capture of extreme ages.\n",
    "\n",
    "Explanations for the differences:\n",
    "Z-score is based on an assumption of normal distribution. if the data are asymmetrical or non-normal, it may miss outliers or detect too few, which is the case in our data. \n",
    "Isolation Forest is more flexible because it does not assume any particular shape for the data. It is therefore more effective at detecting anomalies in skewed or complex data. I choosed contamination=0.1 at first, then I tried 0.05 because I thought that the contamination was too high, Especially far from the center of the distribution.\n",
    "\n",
    "Conclusion:\n",
    "The two methods give a different perspective on outliers. The Z-score is simple but limited for non-normal distributions, while Isolation Forest is more robust for complex structures but I have to find the right paramater. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtLeRqcsQRpB"
   },
   "source": [
    "### 4. Conclusions & Throughts [3]\n",
    "- Disucss methods used for anomaly detection, pros/cons of each method\n",
    "- Disucss challenges/difficulties in anomaly detection implementation\n",
    "- Discuss methods used for imputation, pros/cons of each method\n",
    "- Discuss challenges/difficulties in imputation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coursework allowed us to combine the different methods that we were able to approach in class and tested in practice. Knowing that we saw different methods for each action, we had to make choices.\n",
    "\n",
    "# Anomaly detection choice\n",
    "\n",
    "## Z-score\n",
    "Concerning anomaly detection, I chose z-score as a statistical method and Isolation Forest as an algorithmic method. I chose z-score because it is a simple and quick method to set up, but it is limited by the assumption of normality of the data. For this I tried to use robust z-score that we were able to approach in class.\n",
    "\n",
    "## Isolation Forest\n",
    "Isolation Forest is more robust because it does not make assumptions about the distribution of the data. However, it is more complex to set up and requires finding the right contamination parameter. Isolation Forest is more suitable for small data, which is both an advantage and a disadvantage. I tried to do my best based on my personal research and the knowledge provided in class. I would be very happy to have feedback on my choices and results. I do not know how feedback works in England since I am a French university exchange student here for my third year.\n",
    "\n",
    "# Imputer choice\n",
    "For imputation, I chose KNN and MICE (IterativeImputer). KNN and MICE are both multivariate imputation algorithms. I found some advantages and disadvantages of KNN on this website[1]\n",
    "\n",
    "## KNN Impute:\n",
    "### Advantages\n",
    "- Enhances Data Accuracy (tries to fill NaN with accurate values)\n",
    "- Preserves Data Structure (maintains the relationships and distribution of the data as shown in the above plot)\n",
    "- Handles Numeric Data Effectively (int/float dtypes, where it can make accurate estimations for missing values.)\n",
    "- Integration with Scikit-Learn (easy to integrate with data preprocessing pipeline)\n",
    "### Disadvantages\n",
    "- Sensitive to the Choice of k (selecting an inappropriate value for k may lead to either over-smoothing (too generalized) - or overfitting (too specific) imputations). In our case, we used the default value of k=5. I tried to find the best value for k, trying with 7, 3 and 1, but I found 5 to be the best value.\n",
    "- Highly Computational (can be time-consuming for larger datasets to calculate the distance between two data points)\n",
    "- Handling Categorical Data (imputing discrete values â€‹â€‹can be challenging, but KNN Imputer remains applicable when the data is encoded) In our case, It doesn't matter because we only have numerical data.\n",
    "- Impact of Outliers (too many outliers in the data may lead to wrong imputations)\n",
    "\n",
    "\n",
    "\n",
    "## MICE Impute:\n",
    "Based on these website[2, 3], here are some advantages and disadvantages of the MICE algorithm:\n",
    "### Advantages\n",
    "- Flexibility:\n",
    "MICE (Multiple Imputation by Chained Equations) is highly flexible and can be applied to datasets with various types of variables, including binary, categorical, and continuous data. It allows each variable to be modeled according to its specific distribution, enhancing the accuracy of imputations1.\n",
    "- Handling Complex Datasets:\n",
    "MICE can be used effectively in large datasets with hundreds of variables, making it suitable for complex data structures where traditional joint models may not be appropriate1.\n",
    "- Uncertainty Quantification:\n",
    " By generating multiple imputations for each missing value, MICE accounts for the uncertainty inherent in missing data, leading to more accurate standard errors and reducing the risk of false precision that can occur with single imputation methods3.\n",
    "- Auxiliary Variables:\n",
    " The method allows the inclusion of auxiliary variables that are not part of the main analysis but can improve the quality of imputations by making the Missing At Random (MAR) assumption more plausible1.\n",
    "- Superefficiency: MICE can sometimes provide more precise statistical inferences than maximum likelihood methods by using additional information that may not be accessible to the analyst2.\n",
    "\n",
    "### Disadvantages\n",
    "- Assumption of MAR:\n",
    " MICE assumes that data are Missing At Random (MAR), meaning the probability of missingness depends only on observed values. If this assumption is violated, it can lead to biased estimates13.\n",
    "- Complex Implementation:\n",
    "The method is more complex to implement compared to simpler imputation techniques. It requires careful consideration in setting up the imputation model and validating the quality of generated data24.\n",
    "- Potential for Misleading Results:\n",
    " Without appropriate care and insight, MICE can yield nonsensical or misleading results. It requires scientific and statistical judgment at various stages, such as diagnosing the missing data problem and setting up a robust imputation model2.\n",
    "- Computational Expense:\n",
    " MICE can be computationally expensive, particularly with large datasets or when a high number of imputations is required to achieve stable results4.\n",
    "- Lack of Guidance for Complex Data Structures:\n",
    "There is limited guidance on how to incorporate design factors from complex sampling designs or how to handle nested, hierarchical, or autocorrelated data within the MICE framework2.\n",
    "\n",
    "The main difficulties encountered in implementing imputers was the interpretation of the results. Indeed I think that there are still unknowns in the method to follow for me. I want to practice this field to better understand and deepen the field. For this I am also taking the biomedical and AI course and I will also join the Formula Student club which aims to win a speed race with an autonomous electric vehicle on a circuit.\n",
    "\n",
    "## References\n",
    "1. KNN imputer: https://medium.com/@karthikheyaa/k-nearest-neighbor-knn-imputer-explained-1c56749d0dd7\n",
    "2. MICE: https://www.machinelearningplus.com/machine-learning/mice-imputation/\n",
    "3. MICE: https://pmc.ncbi.nlm.nih.gov/articles/PMC3074241/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
