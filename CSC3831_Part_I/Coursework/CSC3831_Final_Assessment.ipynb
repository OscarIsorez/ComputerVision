{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsFtldtMYMGG"
   },
   "source": [
    "# CSC3831 Final Assessment - Part I: Data Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXEwmOVfYG8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "houses_corrupted = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/CORRUPTED/HOUSES/houses_0.1_MAR.csv', header=0)\n",
    "\n",
    "# for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UkViOchMMIg"
   },
   "source": [
    "Above we've loaded in a corrupted version of a housing dataset. The anomalies need to be dealt with and missing values imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abwbd_vBYsv7"
   },
   "source": [
    "### 1. Data Understanding [7]\n",
    "- Perform ad hoc EDA to understand and describe what you see in the raw dataset\n",
    "  - Include graphs, statistics, and written descritpions as appropriate\n",
    "  - Any extra information about the data you can provide here is useful, think about performing an analysis (ED**A**), what would you find interesting or useful?\n",
    "- Identify features with missing records, outlier records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis is the process of using querying and visualization techniques to examine the surface properties of acquired data. This includes:\n",
    "\n",
    "- Simple statistical analysis\n",
    "- Distribution of attributes\n",
    "- Relationships between pairs or small numbers of attributes\n",
    "\n",
    "First, we are going to check the data itself, to see the shape of the data and the first rows. Then, we are going to use some methods to print tables of information and statistics about the data. We are also going to plot some graphs to visualize the data and make some assumptions about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G04uriMrZH7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(houses_corrupted.head())\n",
    "\n",
    "\n",
    "# print(houses_corrupted.info())\n",
    "print()\n",
    "print(houses_corrupted.describe(include='all'))\n",
    "\n",
    "print(houses_corrupted.info())\n",
    "\n",
    "print(houses_corrupted.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msno.matrix(houses_corrupted)\n",
    "houses_corrupted.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### 1. Data Understanding\n",
    "We can see that the Unnamed: 0 column is not necessary, so we are going to drop it. We can also see that there are some missing values in the data in the median_outcome, housing_median_age and population column. \n",
    "We can see that all column are numerical, which is going to help later on because dealing with categorical data can include \n",
    "additional steps. We have some information about the distribution as well, especially the quartiles, the mean and the standard deviation.\n",
    "\n",
    "### 2. Plots\n",
    "\n",
    "We can see visualy the missing data in 3 columns, we can see the distribution of the data in the columns, and we can see the correlation between the columns.\n",
    "Moreover, some outliers are detected in the data, which we are going to deal with in the next steps, especially because : \n",
    "\n",
    "> Outliers can skew a dataset and influence our measure of centre\n",
    "> and spread. Depending on skew use the metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these findings, we will then proceed to clean the remove the useless data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    houses_corrupted.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"already dropped\")\n",
    "\n",
    "sns.histplot(houses_corrupted['median_house_value'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no string features so we can skip the encoding part, and we don't have to worry about the categorical data. We will only focus on the numerical data. We wont need to look for malformed strings too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to make a pairplot to look for any relationship between the features. We are going to use the seaborn library to make the pairplot. It is also going to be useful to describe the distribution of the data, the skewness, and the outliers.\n",
    "\n",
    "> Def:\n",
    "> Pairs plots are a series of\n",
    "> scatter plots of every attribute\n",
    "> present in the data.\n",
    "> ‚Ä¢ Useful for an initial look at\n",
    "> relationships in the data\n",
    "> ‚Ä¢ Visually overwhelming for\n",
    "> final reports\n",
    "> ‚Ä¢ Utilise to detect interesting\n",
    "> relationships to analyse further\n",
    "\n",
    "1_Introduction_to_Data_Science.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_corrupted)\n",
    "print(houses_corrupted.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results :\n",
    "\n",
    "We can see that there is a strong correlation between the total_bedroom and housholds. Some other features are also strongly corrolated with each other as we can see on this pairplot. \n",
    "\n",
    "In the diagonal, we can see the distribution of the data. We can see that the data is not normally distributed, and we can see that there are some outliers in the data. We can see that the data is positively skewed,like the figure 2.1 of 1_Introduction_to_Data_Science.pdf show. This is going to be useful to know for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Imputation [10]\n",
    "- Identify which features should be imputed and which should be removed\n",
    "  - Provide a written rationale for this decision\n",
    "- Impute the missing records using KNN imputation\n",
    "- Impute the missing records using MICE imputation\n",
    "- Compare both imputed datasets feature distributions against each other and the non-imputed data\n",
    "- Build a regressor on all thre datasets\n",
    "  - Use regression models to predict house median price\n",
    "  - Compare regressors of non-imputed data against imputed datas\n",
    "  - **Note**: If you're struggling to compare against the original dataset focus on comparing the two imputed datasets against each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "# Identify features to impute or remove\n",
    "missing_threshold = 0.3\n",
    "features_to_impute = []\n",
    "features_to_remove = []\n",
    "\n",
    "for column in houses_corrupted.columns:\n",
    "    missing_percentage = houses_corrupted[column].isnull().sum() / houses_corrupted.shape[0]\n",
    "    if missing_percentage > missing_threshold:\n",
    "        features_to_remove.append(column)\n",
    "    elif missing_percentage > 0:\n",
    "        features_to_impute.append(column)\n",
    "\n",
    "print(\"Features to impute:\", features_to_impute)\n",
    "print(\"Features to remove:\", features_to_remove)\n",
    "\n",
    "houses_corrupted = houses_corrupted.drop(columns=features_to_remove)\n",
    "\n",
    "X = houses_corrupted.drop(columns=['median_house_value'])\n",
    "y = houses_corrupted['median_house_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_knn_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_test_knn_imputed = knn_imputer.transform(X_test)\n",
    "\n",
    "mice_imputer = IterativeImputer(random_state=SEED)\n",
    "X_train_mice_imputed = mice_imputer.fit_transform(X_train)\n",
    "X_test_mice_imputed = mice_imputer.transform(X_test)\n",
    "\n",
    "def plot_feature_distributions(original, knn_imputed, mice_imputed, feature_names):\n",
    "    fig, axes = plt.subplots(len(feature_names), 3, figsize=(15, len(feature_names) * 5))\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        sns.histplot(original[feature], ax=axes[i, 0], kde=True, color='blue')\n",
    "        axes[i, 0].set_title(f'Original {feature}')\n",
    "        sns.histplot(knn_imputed[:, i], ax=axes[i, 1], kde=True, color='green')\n",
    "        axes[i, 1].set_title(f'KNN Imputed {feature}')\n",
    "        sns.histplot(mice_imputed[:, i], ax=axes[i, 2], kde=True, color='red')\n",
    "        axes[i, 2].set_title(f'MICE Imputed {feature}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distributions(X_train, X_train_knn_imputed, X_train_mice_imputed, features_to_impute)\n",
    "\n",
    "def build_and_evaluate_regressor(X_train, X_test, y_train, y_test):\n",
    "    # regressor = DecisionTreeRegressor(random_state=SEED)\n",
    "    # regressor = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, r2\n",
    "\n",
    "X_train_original = X_train.dropna()\n",
    "y_train_original = y_train[X_train_original.index]\n",
    "X_test_original = X_test.dropna()\n",
    "y_test_original = y_test[X_test_original.index]\n",
    "\n",
    "mse_original, r2_original = build_and_evaluate_regressor(X_train_original, X_test_original, y_train_original, y_test_original)\n",
    "print(f'Original Data - MSE: {mse_original}, R2: {r2_original}')\n",
    "\n",
    "# KNN Imputed data\n",
    "mse_knn, r2_knn = build_and_evaluate_regressor(X_train_knn_imputed, X_test_knn_imputed, y_train, y_test)\n",
    "print(f'KNN Imputed Data - MSE: {mse_knn}, R2: {r2_knn}')\n",
    "\n",
    "# MICE Imputed data\n",
    "mse_mice, r2_mice = build_and_evaluate_regressor(X_train_mice_imputed, X_test_mice_imputed, y_train, y_test)\n",
    "print(f'MICE Imputed Data - MSE: {mse_mice}, R2: {r2_mice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Interpretation\n",
    "We can see that the KNN got a very similar distribution to the original data, more that MICE. \n",
    "\n",
    "### Imputation\n",
    "\n",
    "KNN/MICE. The two imputer got a pretty similar result, which are quite good, compared to the original data.\n",
    "\n",
    "### Regression\n",
    "I tried to use DecisionTree, RandomForest and LinearRegression to predict the median_house_value. They got pretty similar results, so I kept to LinearRegressor because as we discussed in class, it allows use to keep the intepretation without the complexity of the other models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR74DAF_ZQUy"
   },
   "source": [
    "### 2. Outlier Identification [10]\n",
    "- Utilise a statistical outlier detection approach (i.e., **no** KNN, LOF, 1Class SVM)\n",
    "- Utilise an algorithmic outlier detection method of your choice\n",
    "- Compare results and decide what to do with identified outleirs\n",
    "  - Include graphs, statistics, and written descriptions as appropriate\n",
    "- Explain what you are doing, and why your analysis is appropriate\n",
    "- Comment on benefits/detriments of statistical and algorithmic outlier detection approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers in a dataset are either:\n",
    "- Rare: Appear with low frequency relative to the rest of the data (inliers)\n",
    "- Unusual: Do not fit the data distribution\n",
    "\n",
    "\n",
    "Missing at Random (MAR) is data where there may be a systemic\n",
    "reason why some of the data is missing, but this knowledge does\n",
    "not help us with imputation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing I think that dealing the missing values in this section is relevant. In fact we can see that there are some missing values in the data, and we are going to use the KNN imputer to fill the missing values. We are going to use the KNN imputer because it is a good imputer for numerical data, and it is going to take into account the correlation between the features. Moreover, We need data without missing values to detect the outliers, so I feel like it is a good idea to fill the missing values before detecting the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dataset for comparison against the imputed datasets\n",
    "houses = pd.read_csv('https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/ORIGINAL/houses.csv', header=0)\n",
    "\n",
    "original_columns = houses_corrupted.columns\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "houses_corrupted = pd.DataFrame(knn_imputer.fit_transform(houses_corrupted), columns=original_columns)\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print( houses_corrupted.isnull().sum() )\n",
    "\n",
    "houses_corrupted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADs and Z-scores\n",
    "\n",
    "We are now going to use the robust Z-score as discussed in the lecture because the data is skewed and include outliers\n",
    "\n",
    "> MADs Def: \n",
    "> Median of all absolute\n",
    "> deviations from the median\n",
    "> ùëÄùê¥ùê∑ = 1.483 ‚àó ùëöùëíùëëùëñ:1‚Ä¶ùëõ(|ùë•ùëñ ‚àí ùëöùëíùëë(ùë•ùëó)ùëó:1‚Ä¶ùëõ|)\n",
    "\n",
    "> Z-score Def:\n",
    "> Z-score is a conversion of standard deviation from a normal distribution to a\n",
    "> standard normal distribution.\n",
    "\n",
    "\n",
    "ùëã = ùë•1, ‚Ä¶ , ùë•ùëõ\n",
    "ùëüùëúùëè_ùëßùëñ =\n",
    "ùë•ùëñ ‚àí ùëöùëíùëë(ùë•)\n",
    "ùëÄùê¥ùê∑\n",
    "\n",
    "\n",
    "I thought multiple time to scale the data using the standard scaler, but I decided not to do it and work with the raw data. In fact, scaling skewed data is not very effective. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPsaKYCZZPkv"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "columns_to_analyze = ['population', 'median_income', 'housing_median_age']\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    # ùëã = {ùë•1, ‚Ä¶ , ùë•ùëõ}  ùëüùëúùëè_ùëßùëñ = (ùë•ùëñ ‚àí ùëöùëíùëë(ùë•) )/ ùëÄùê¥ùê∑\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    robust_z_scores = 0.6745 * (data - median) / mad\n",
    "    return np.abs(robust_z_scores) > threshold\n",
    "\n",
    "def detect_outliers_isolation_forest(data):\n",
    "    isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    outliers = isolation_forest.fit_predict(data)\n",
    "    return outliers == -1\n",
    "\n",
    "def compare_outliers(data, columns):\n",
    "    fig, axes = plt.subplots(len(columns), 2, figsize=(15, len(columns) * 5))\n",
    "    for i, column in enumerate(columns):\n",
    "        # D√©tection des valeurs aberrantes\n",
    "        outliers_zscore = detect_outliers_zscore(data[column])\n",
    "        outliers_iforest = detect_outliers_isolation_forest(data[[column]])\n",
    "        \n",
    "        sns.histplot(data[column], ax=axes[i, 0], kde=True, color='blue')\n",
    "        sns.histplot(data[column][outliers_zscore], ax=axes[i, 0], kde=True, color='red')\n",
    "        axes[i, 0].set_title(f'{column} - Z-score')\n",
    "        axes[i, 0].text(0.05, 0.95, 'Blue: Original\\nRed: Outliers', transform=axes[i, 0].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))\n",
    "        \n",
    "        sns.histplot(data[column], ax=axes[i, 1], kde=True, color='blue')\n",
    "        sns.histplot(data[column][outliers_iforest], ax=axes[i, 1], kde=True, color='red')\n",
    "        axes[i, 1].set_title(f'{column} - Isolation Forest')\n",
    "        axes[i, 1].text(0.05, 0.95, 'Blue: Original\\nRed: Outliers', transform=axes[i, 1].transAxes,\n",
    "                        fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_outliers(houses_corrupted, columns_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : \n",
    "###  Chart analysis : \n",
    "\n",
    "Population :\n",
    "\n",
    "Z-score: Outliers are mainly located on the right of the distribution (very high values).\n",
    "Isolation Forest: Outliers are similar but slightly more numerous, with slightly wider detection.\n",
    "Median Income :\n",
    "\n",
    "Z-score: Only very high values are detected as outliers.\n",
    "Isolation Forest: Detection is more diverse, with anomalies in the high and low parts, indicating that it considers very low and very high incomes to be abnormal.\n",
    "\n",
    "Housing Median Age :\n",
    "Z-score: Detects few anomalies, only at the right end (high values).\n",
    "Isolation Forest: More diversity in anomalies, with better capture of extreme ages.\n",
    "\n",
    "Explanations for the differences:\n",
    "Z-score is based on an assumption of normal distribution. if the data are asymmetrical or non-normal, it may miss outliers or detect too few, which is the case in our data. \n",
    "Isolation Forest is more flexible because it does not assume any particular shape for the data. It is therefore more effective at detecting anomalies in skewed or complex data. I choosed contamination=0.1 at first, then I tried 0.05 because I thought that the contamination was too high, Especially far from the center of the distribution.\n",
    "\n",
    "Conclusion:\n",
    "The two methods give a different perspective on outliers. The Z-score is simple but limited for non-normal distributions, while Isolation Forest is more robust for complex structures but I have to find the right paramater. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtLeRqcsQRpB"
   },
   "source": [
    "### 4. Conclusions & Throughts [3]\n",
    "- Disucss methods used for anomaly detection, pros/cons of each method\n",
    "- Disucss challenges/difficulties in anomaly detection implementation\n",
    "- Discuss methods used for imputation, pros/cons of each method\n",
    "- Discuss challenges/difficulties in imputation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce coursework a permis de combiner les diff√©rentes m√©thodes que nous avons pu aborder en cours et test√© en practical. Sachant que nous avons vu diff√©rentes m√©thodes pour chaque actions, il a donc fallu faire des choix. \n",
    "\n",
    "Concernant l'anomaly detection, j'ai choisi z-score comme m√©thode statistique et Isolation Forest comme m√©thode algorithmique. J'ai choisi z-score car c'est une m√©thode simple et rapide √† mettre en place, mais elle est limit√©e par l'assumption de normalit√© des donn√©es. Pour cela j'ai tent√© d'utiliser robust z-score que nous avons pu aborder en classe.\n",
    "\n",
    "Isolation Forest est plus robuste car elle ne fait pas d'assumption sur la distribution des donn√©es. Cependant, elle est plus complexe √† mettre en place et n√©cessite de trouver le bon param√®tre de contamination. Isolation Forest est plus adapt√©e pour des donn√©es de petite taille, ce qui est √† la fois un avantage et un inconv√©nient. J'ai tent√© de faire au mieux √† partir de mes recherches personnelle et les connaissances fournies en cours. Je serai tr√®s heureux d'avoir des retours sur mes choix et mes r√©sultats. Je ne sais pas comment fonctionne les retours en angleterre √©tant donn√© que je suis un √©tudiant en √©change universitaire fran√ßais venu pour ma troisi√®me ann√©e.\n",
    "\n",
    "Pour l'imputation, j'ai choisi KNN et MICE (IterativeImputer). KNN and MICE are both multivariate imputation algorithms. I found some advantages and disadvantages of KNN on this website[1]\n",
    "\n",
    "## Advantages\n",
    "- Enhances Data Accuracy (tries to fill NaN with accurate values)\n",
    "- Preserves Data Structure (maintains the relationships and distribution of the data as shown in the above plot)\n",
    "- Handles Numeric Data Effectively (int/float dtypes, where it can make accurate estimations for missing values.)\n",
    "- Integration with Scikit-Learn (easy to integrate with data preprocessing pipeline)\n",
    "## Disadvantages\n",
    "- Sensitive to the Choice of k (selecting an inappropriate value for k may lead to either over-smoothing (too generalized) - or overfitting (too specific) imputations). In our case, we used the default value of k=5. I tried to find the best value for k, trying with 7, 3 and 1, but I found 5 to be the best value.\n",
    "- Highly Computational (can be time-consuming for larger datasets to calculate the distance between two data points)\n",
    "- Handling Categorical Data (imputing discrete values can be challenging, but KNN Imputer remains applicable when the data is encoded) In our case, It doesn't matter because we only have numerical data.\n",
    "- Impact of Outliers (too many outliers in the data may lead to wrong imputations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "1. KNN imputer : https://medium.com/@karthikheyaa/k-nearest-neighbor-knn-imputer-explained-1c56749d0dd7\n",
    "2. MICE : https://www.machinelearningplus.com/machine-learning/mice-imputation/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
