{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIE9-BRo5IYy"
   },
   "source": [
    "# Practical 2\n",
    "\n",
    "## Preliminary\n",
    "\n",
    "Load the required libraries and set defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D3LsvXm67XNx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier,\n",
    "    KNeighborsRegressor,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,\n",
    "    plot_tree,\n",
    ")\n",
    "\n",
    "sns.set_theme()\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0RpYViR2V74"
   },
   "source": [
    "## Part 1: Validation Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2kw52KXAS_7"
   },
   "source": [
    "We will explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "We use the function `train_test_split()` to split the data into training and validation sets. As there are $392$ observations, we split into two equal sets of size $196$ using the argument `test_size=196`.\n",
    "\n",
    "It is generally a good idea to set a random seed when performing operations like this that contain an element of randomness, so that the results obtained can be reproduced precisely at a later time. We set the random seed of the splitter with the argument `random_state=seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WgvssbBE2YJj"
   },
   "outputs": [],
   "source": [
    "# load data set\n",
    "path = 'https://github.com/vladoxNCL/ml_course/raw/main/Auto.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# change horsepower type to float and fix missing values\n",
    "df['horsepower'] = df['horsepower'].replace('?', np.nan)\n",
    "df['horsepower'] = df['horsepower'].astype(float)\n",
    "hp_median = df['horsepower'].median()\n",
    "df['horsepower'] = df['horsepower'].fillna(hp_median)\n",
    "\n",
    "# add quadratic hp feature\n",
    "df['horsepower^2'] = df['horsepower'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgnJATnrBGjY"
   },
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `df_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o8jYJUcuBF4E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 19.035743133470827\n",
      "Validation MSE: 20.673515479990726\n"
     ]
    }
   ],
   "source": [
    "predictors = ['horsepower', 'horsepower^2']\n",
    "response = 'mpg'\n",
    "reg = LinearRegression()\n",
    "\n",
    "# split data into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    df[predictors],\n",
    "    df[response],\n",
    "    test_size=196,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "y_train_pred = reg.predict(X_train)\n",
    "y_valid_pred = reg.predict(X_valid)\n",
    "\n",
    "print(f'Training MSE: {mean_squared_error(y_train, y_train_pred)}')\n",
    "print(f'Validation MSE: {mean_squared_error(y_valid, y_valid_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4taZ3x4IE7Bj"
   },
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Lets now check the MSE using 5-Fold, 10-Fold, and Leave-One-Out (LOO) cross-validation.\n",
    "\n",
    "For this, we will use `sklearn`'s `cross_val_score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HeMmcYMxEO7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold-Fold MSE: 25.224451012588855\n",
      "10-Fold-Fold MSE: 22.13654485642315\n",
      "LOO-Fold MSE: 20.019475910738382\n"
     ]
    }
   ],
   "source": [
    "folds = {\n",
    "    '5-Fold': 5,\n",
    "    '10-Fold': 10,\n",
    "    'LOO': df.shape[0],\n",
    "}\n",
    "\n",
    "for key, val in folds.items():\n",
    "    scores = cross_val_score(\n",
    "        reg, df[predictors], df[response],\n",
    "        cv=val, scoring='neg_mean_squared_error',\n",
    "    )\n",
    "    print(f'{key}-Fold MSE: {-scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP9avqUOE2fR"
   },
   "source": [
    "The CV approach gives a better estimation of real-world test performance, as all of the data is being utilised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebsuoc-KGonK"
   },
   "source": [
    "\n",
    "### Hands On\n",
    "\n",
    "For the `Credit` data set, predict `Balance` from the remaining features using KNN regression. Some of these features are categorical, and will need to be (one-hot) encoded first.\n",
    "\n",
    "Use **5-Fold cross-validation** to find the best $K \\in \\{1, 2, \\dots, 10\\}$ with respect to **R^2 Score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NJpVIT41Gmo6"
   },
   "outputs": [],
   "source": [
    "path = 'https://github.com/vladoxNCL/ml_course/raw/main/Credit.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbKb0SX6dPl"
   },
   "source": [
    "## Part 2: Classification\n",
    "\n",
    "In class, we learnt about a few different classification models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. KNN\n",
    "3. Na√Øve Bayes\n",
    "4. Decision trees\n",
    "5. Ensemble methods (e.g., random forests)\n",
    "6. SVCs\n",
    "\n",
    "Lets compare their performance on the `Default` dataset to predict `default` from `student`, `balance`, and `income`, using 5-Fold CV to select the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FPDGDNdjLBFH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'https://github.com/vladoxNCL/ml_course/raw/main/Default.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "for col in ['default', 'student']:\n",
    "    df[col] = df[col].map({'No': 0, 'Yes': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OAg-Wc7WLfNo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy\t Train CV: 0.974\t Test: 0.968\n",
      "KNN Accuracy\t Train CV: 0.970\t Test: 0.962\n",
      "GNB Accuracy\t Train CV: 0.971\t Test: 0.958\n",
      "DT Accuracy\t Train CV: 0.955\t Test: 0.953\n",
      "RF Accuracy\t Train CV: 0.969\t Test: 0.963\n",
      "SVC Accuracy\t Train CV: 0.973\t Test: 0.962\n"
     ]
    }
   ],
   "source": [
    "preds = ['student', 'balance', 'income']\n",
    "response = 'default'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[preds],\n",
    "    df[response],\n",
    "    test_size=0.1,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "clfs = {\n",
    "    'LR': make_pipeline(StandardScaler(), LogisticRegression(random_state=seed)),\n",
    "    'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    'GNB': make_pipeline(StandardScaler(), GaussianNB()),\n",
    "    'DT': DecisionTreeClassifier(random_state=seed),\n",
    "    'RF': RandomForestClassifier(random_state=seed),\n",
    "    'SVC': make_pipeline(StandardScaler(), SVC(random_state=seed)),\n",
    "}\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    scores = cross_val_score(\n",
    "        clf, X_train, y_train,\n",
    "        cv=5, scoring='accuracy',\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    accu = accuracy_score(y_test, preds)\n",
    "    print(f'{clf_name} Accuracy\\t Train CV: {scores.mean():.3f}\\t Test: {accu:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WOZgQ2gMgQ9"
   },
   "source": [
    "All the models perform really well. However, the edge goes to **logistic regression**, since it is the most interpretable.\n",
    "\n",
    "\n",
    "### Hyper-parameter tuning\n",
    "\n",
    "As an exercise, lets try to improve the accuracy of **random forests** and **SVMs** through hyper-parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-AgExl_RMYH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'n_estimators': 8}\n",
      "Best CV Score: 0.9732222222222223\n",
      "Test Accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "# set up the model and hyperparameter grid\n",
    "rf_clf = RandomForestClassifier(random_state=seed)\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(1, 10)),\n",
    "    'max_depth': list(range(1, 10)),\n",
    "}\n",
    "\n",
    "# perform cross-validation and hyperparameter tuning on the training set\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# best hyperparameters and cross-validation score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best CV Score: {grid_search.best_score_}')\n",
    "print(f'Test Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s1UkxbMF8F0i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svc__C': 10, 'svc__kernel': 'rbf'}\n",
      "Best CV Score: 0.9732222222222223\n",
      "Test Accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "svc_clf = make_pipeline(StandardScaler(), SVC(random_state=seed))\n",
    "param_grid = {\n",
    "    'svc__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# perform cross-validation and hyperparameter tuning on the training set\n",
    "grid_search = GridSearchCV(svc_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters and cross-validation score\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best CV Score: {grid_search.best_score_}')\n",
    "print(f'Test Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cDmhMkaWfoX"
   },
   "source": [
    "Hyper-parameter tuning improved the performance of both RF and SVC. However, their performance didn't match **logistic regression**, which should still be preferred due to its interpretability.\n",
    "\n",
    "Lets see what info we can get from the LR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vPDYP2Lj-KqJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "student: -3.844\n",
      "balance: 0.004\n",
      "income: -0.000\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=seed)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print('Coefficients:')\n",
    "for feat, coef in zip(X_test.columns, clf.coef_[0]):\n",
    "    print(f'{feat}: {coef:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP6xcVf3_fzR"
   },
   "source": [
    "This tells us that `student` is **negatively** correlated with defaulting, while `balance` is **positively** correlated with defaulting (although to a lesser extent), and `income` is *uncorrelated* with defaulting.\n",
    "\n",
    "Another explainable model is generated by decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XX-EnoNDAFZ4"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3, random_state=seed)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(f'Decision Tree Accuracy: {accuracy_score(y_test, preds)}\\n')\n",
    "\n",
    "imps = clf.feature_importances_\n",
    "print('Feature Importances:')\n",
    "for feat, imp in zip(X_train.columns, imps):\n",
    "    print(f'{feat}\\t {imp:.4f}')\n",
    "print('')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_tree(clf, feature_names=X_train.columns, filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ERBuJwBPix"
   },
   "source": [
    "For this model, `balance` is the most important feature, followed by `income` and finally `student` is not important.\n",
    "\n",
    "The colours in the plot represent the **classes**: *red* means $0$ (don't default) and *blue* means $1$ (default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crxgOFYYU24a"
   },
   "source": [
    "## Hands On\n",
    "\n",
    "Use the *Wine* dataset from scikit-learn and the Support Vector Classifier (SVC) model to perform 10-fold cross-validation.\n",
    "\n",
    "1. Load the Wine dataset from *scikit-learn*.\n",
    "2. Train/test split your data with a 90/10 proportion.\n",
    "3. Create an SVC classifier.\n",
    "4. Perform 10-fold cross-validation on the *training* data.\n",
    "5. Print the cross-validation scores and the mean score.\n",
    "6. Use the `GridSearchCV` function to tune the `'C'`, `'kernel'`, `'gamma'`, and `'degree'` hyper-parameters.\n",
    "7. Verify the quality of the resulting hyper-parameters over the *test* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ftBi58ANuuE"
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "wine.keys()\n",
    "\n",
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1_8ODB5_nC0p9mh9wl3wGY3xxD5L_rLV5",
     "timestamp": 1729851816516
    },
    {
     "file_id": "1dW5MRRaLDxPEXYiz3xFiIYSIw9F5SiGI",
     "timestamp": 1729759927294
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
