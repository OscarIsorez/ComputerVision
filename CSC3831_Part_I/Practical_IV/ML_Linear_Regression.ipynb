{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MbKYnFt_-nUVU9t2Rywgcb6A-fjgb0YS","timestamp":1727964224138},{"file_id":"1MaDR_VV7m-RTiCs9oswDbML20AUwrffw","timestamp":1727780758619}],"authorship_tag":"ABX9TyPdF5FRNZRZMS08CI/hbpdN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ML Practical 1\n","\n","This practical session is divided into 2 sections:\n","\n","1. Linear Regression\n","2. Validation Approaches"],"metadata":{"id":"eF9eXdb7NgME"}},{"cell_type":"markdown","source":["## 0. Preliminary\n","\n","To begin, lets load the packages that we will use. These are:\n","- For **loading** and **transforming** our data, we will use `pandas` and `numpy`\n","- For our **modelling**, we will use several `sklearn` sub-modules\n","- For **plotting**, we will use `seaborn` and `matplotlib`"],"metadata":{"id":"HnvO-lS-Ldbq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"67YJzVguMlaH"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.linear_model import (\n","    LinearRegression,\n","    LogisticRegression,\n",")\n","\n","from sklearn.metrics import (\n","    classification_report,\n","    accuracy_score,\n","    root_mean_squared_error,\n","    r2_score,\n","    confusion_matrix,\n",")\n","\n","from sklearn.model_selection import (\n","    train_test_split,\n","    cross_val_score,\n","    GridSearchCV,\n",")\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","sns.set_theme()"]},{"cell_type":"markdown","source":["## 1. Linear Regression\n","\n","In the first part, we will load the `Boston` housing data set, and fit both a simple linear regression model with a single predictor, as well as a multinomial regression model, using additional variables.\n","\n","We load the `Boston` dataset from the module's repository using pandas' `load_csv()` method.\n","\n","Then, we print the first 10 rows of data using the `head()` method."],"metadata":{"id":"joH2EZDiPFxr"}},{"cell_type":"code","source":["path = 'https://github.com/vladoxNCL/ml_course/raw/main/Boston.csv'\n","df = pd.read_csv(path)\n","df.head(10)"],"metadata":{"id":"qWViH8i_PE9Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the first column is unnecessary, as it is just an index. We can *drop* it using pandas' `iloc` method and overwriting `df`:"],"metadata":{"id":"lj12j07iP1eg"}},{"cell_type":"code","source":["df = df.iloc[:, 1:]\n","df.head(10)"],"metadata":{"id":"2VmP_CEgPxWr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will first fit a simple linear regression model, using `lstat` as the predictor and `medv` as the response."],"metadata":{"id":"4DjcKzgoTZOF"}},{"cell_type":"code","source":["X = df[['lstat']]\n","y = df['medv']\n","\n","reg = LinearRegression()\n","reg.fit(X, y)\n","\n","coef = reg.coef_[0]\n","inter = reg.intercept_\n","\n","print(f'Coefficient: {coef}')\n","print(f'Intercept: {inter}')"],"metadata":{"id":"_UGRlACUTYXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets plot both the data and the fitted regression line"],"metadata":{"id":"qpyB1Xb2gsgl"}},{"cell_type":"code","source":["# Scatter plot of the data\n","sns.scatterplot(x=df['lstat'], y=df['medv'])\n","\n","# Generate and plot the regression line\n","xmin, xmax= df['lstat'].min(), df['lstat'].max()\n","x_plot = np.linspace(xmin, xmax, 100)\n","y_plot = coef * x_plot + inter\n","sns.lineplot(x=x_plot, y=y_plot, color='red');"],"metadata":{"id":"UvTi3IB3QEa9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To test how good the model is at predicting *unknown* data, lets perform a train-test split, fit a model to the *training* data and then measure the model accuracy on the *test* set:"],"metadata":{"id":"mQFpnQ48g0Zf"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","rmse = root_mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(\"RMSE:\", rmse)\n","print(\"R2 Score:\", r2)"],"metadata":{"id":"dKdVM1ejWaXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multiple Linear Regression\n","\n","Lets now consider using 2 predictors instead of just one: `age` and `lstat` and see how that impacts on the model performance.\n","\n","When you notice your code is repetitive, it makes sense to wrap it in a **function** so that you can reuse it later. We will define a `fit_model` function that returns all the variables of interest to us:"],"metadata":{"id":"3bSfKwGsZtLU"}},{"cell_type":"code","source":["def fit_model(data, preds, response):\n","    X = data[preds]\n","    y = data[response]\n","\n","    reg = LinearRegression()\n","    reg.fit(X, y)\n","\n","    coef = reg.coef_\n","    inter = reg.intercept_\n","\n","    print(f'Coefficients: {coef}')\n","    print(f'Intercept: {inter}')\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    reg.fit(X_train, y_train)\n","    y_pred = reg.predict(X_test)\n","    rmse = root_mean_squared_error(y_test, y_pred)\n","    r2 = r2_score(y_test, y_pred)\n","\n","    print(f'RMSE: {rmse:.3f}')\n","    print(f'R2 Score: {r2:.3f}')\n","    return coef, inter"],"metadata":{"id":"qUgRaYgpaos0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now use our function to try out different models very easily:"],"metadata":{"id":"85_ssDnnhifW"}},{"cell_type":"code","source":["preds = ['age', 'lstat']\n","response = 'medv'\n","\n","fit_model(df, preds, response)"],"metadata":{"id":"eW5ya_5OYmUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Adding `age` didn't improve the predictions much. Lets instead add a quadratic `lstat` transformation to the model:"],"metadata":{"id":"GRxbtjSwby_a"}},{"cell_type":"code","source":["df['lstat^2'] = df['lstat'] ** 2\n","\n","preds = ['lstat', 'lstat^2']\n","response = 'medv'\n","\n","coef, inter = fit_model(df, preds, response)"],"metadata":{"id":"2s600tgVbswl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a much better fit! Lets plot the fitted curve\n","$$\n","\\text{medv} = -2.333 \\cdot \\text{lstat} + 0.044 \\cdot \\text{lstat}^2 + 42.862\n","$$"],"metadata":{"id":"mx_35jdOh3Yg"}},{"cell_type":"code","source":["# Scatter plot of the data\n","sns.scatterplot(x=df['lstat'], y=df['medv'])\n","\n","# Add quadratic regression curve\n","xmin, xmax = df['lstat'].min(), df['lstat'].max()\n","x_plot = np.linspace(xmin, xmax, 100)\n","y_plot = coef[0] * x_plot + coef[1] * x_plot ** 2 + inter\n","sns.lineplot(x=x_plot, y=y_plot, color='red');"],"metadata":{"id":"_h1W6asXcF-m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now try it yourself!\n","\n","Load the `Auto` dataset from the module repository, and fit a simple regression model using `mpg` as response and `horsepower` as the predictor.\n","\n","Try performing some transformations over `horsepower` and see if adding them to the model improves its performance."],"metadata":{"id":"Ca46fpvUf40N"}},{"cell_type":"code","source":["path = 'https://github.com/vladoxNCL/ml_course/raw/main/Auto.csv'\n"],"metadata":{"id":"j8ENuKWn2dUA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hands On Extra\n","\n","This question should be answered using the `Carseats` data set.\n","\n","1. Fit a multiple regression model to predict `Sales` using `Price`, `Urban`, and `US`\n","2. Provide an interpretation of each coefficient in the model. Be careful, some of the variables in the model are qualitative.\n","3. Write out the model in equation form, being careful to handle the qualitative variables properly.\n","4. How well does the model fit the data?"],"metadata":{"id":"Foh4eISKuiv1"}},{"cell_type":"code","source":[],"metadata":{"id":"Rr88F9i3vGxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXTRA: Using `statsmodels`\n","\n","An alternative to `sklearn` is `statsmodels`. This package has a much narrower selection of models, but it provides with a lot of additional stats about the model that might come in handy at analysing variable importances (using the variables' $p$-values) and plotting confidence intervals. The following example gives you an idea of how to use this package."],"metadata":{"id":"OIvIc3tKZ5pu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOSthRXWYxKw"},"outputs":[],"source":["# import statsmodels\n","\n","import statsmodels.api as sm\n","from statsmodels.sandbox.regression.predstd import wls_prediction_std\n","from statsmodels.tools.eval_measures import mse"]},{"cell_type":"markdown","source":["Lets generate some sythetic data from the following relationship:\n","\n","$$\n","    y = 0.5 + x + x^3 + \\epsilon\n","$$\n","\n","Notice that there is no $x^2$ term in the equation. This should be detected by the $p$-value for $x^2$ in the resulting model (a predictor is important only if its associated $p$-value is **very** close to $0$)"],"metadata":{"id":"yP3UyxYGlaHA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvjm3u7tYxKw"},"outputs":[],"source":["# random number generator\n","rng = np.random.default_rng(seed=12345)\n","\n","# generate X\n","n_sample = 100\n","x = rng.uniform(-1, 1, n_sample)\n","x = np.sort(x)\n","X = np.column_stack((x, x**2, x**3))\n","\n","# add column of 1s for intercept\n","X = sm.add_constant(X)\n","\n","# generate y\n","beta = np.array([0.5, 1, 0, 2])\n","y_gen = np.dot(X, beta)\n","\n","# add random noise to data (the epsilon term)\n","e = 0.5 * rng.normal(size=n_sample)\n","y = y_gen + e"]},{"cell_type":"markdown","source":["Now, lets fit our model using `statsmodels`, and print a summary of it.\n","\n","Note that in `statsmodels`, linear regression is called `OLS`, short for *ordinary least squares*"],"metadata":{"id":"oH9u3xkZnDRs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qR3v7h_BhixG"},"outputs":[],"source":["# fit model and print the summary\n","model = sm.OLS(y, X)\n","res = model.fit()\n","print(res.summary())"]},{"cell_type":"markdown","source":["The resulting model is:\n","\n","$$\n","    \\hat{y} = 0.5241 + 1.1211 x + 0.0060 x^2 + 1.8838 x^3\n","$$\n","\n","- Notice that the coefficient for $x^2$ is quite small.\n","- Also notice that `x2`, the variable associated with $x^2$, has a relatively **high** $p$-value (shown in the `P>|t|` column).\n","- This means that the variable is not important, and should be removed from the model.\n","\n","Lets do it:"],"metadata":{"id":"u03fK2IYnZY9"}},{"cell_type":"code","source":["X_new = X[:, [0, 1, 3]] # ignore the 2nd column (the one associated with x^2)\n","model = sm.OLS(y, X_new)\n","res = model.fit()\n","print(res.summary())"],"metadata":{"id":"CfgMmDhvn8ir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While the `R-squared` variable is very similar for both models, there are two reasons why we should prefer this second model:\n","1. The **parsimony** principle (or *Occam's Razor*), by which we prefer *simpler* models if they perform about as well as a more complex one.\n","2. the `Prob (F-statistic)` statistic is much lower for the second model, indicating that it is actually a better fit.\n","\n","The resulting model is:\n","\n","$$\n","    \\hat{y} = 0.5260 + 1.1215 x + 1.8832 x^3\n","$$\n","\n","which is a good estimate of the data-generating function:\n","\n","$$\n","    y = 0.5 + x + 2 x^3\n","$$"],"metadata":{"id":"yHYLMcNLoXsx"}},{"cell_type":"markdown","source":["### Getting Confidence Intervals\n","\n","Finally, lets use the `wls_prediction_std` function to generate a $95\\%$ confidence intervals (CIs) for our model. This means that we expect that 95 out of 100 points will fall between these CIs."],"metadata":{"id":"THSmvVFnsDWg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IrFjSMHhixH"},"outputs":[],"source":["prstd, iv_l, iv_u = wls_prediction_std(res)"]},{"cell_type":"markdown","source":["By plotting everything together, we can verify that our fitted model is really close to the generating function, and that the CI *does* contain most of the actual data:"],"metadata":{"id":"buZrNQacrxJ8"}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(8,6))\n","\n","ax.plot(x, y, 'o', label='Generated Data')\n","ax.plot(x, y_gen, 'b-', label='Generating Function')\n","ax.plot(x, res.predict(), 'r--.', label='Fitted Model')\n","ax.plot(x, iv_u, 'g--', label='95% Confidence Interval')\n","ax.plot(x, iv_l, 'g--')\n","ax.legend(loc='best');"],"metadata":{"id":"SQg2EUOrotTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hands On - Feature Selection\n","\n","This method is known as **backward feature selection**, and is a way to obtain a more *parsimonious* model:\n","\n","1. Using `statsmodels`, fit a linear regression model for the `Boston` data set, with `medv` as the response and all the remaining columns as predictors.\n","2. Based on the **model summary**, remove the predictor with the *highest* $p$-value, and fit the model again.\n","3. Keep doing this until all of the remaining predictors have a $p \\le 0.05$ or another threshold, e.g., $0.01$.\n","\n","Alternatively you could also perform **forward feature selection**:\n","\n","1. If you have $m$ predictors, generate $m$ **simple linear regression** models, each with a single predictor, and select the one with the **lowest** MSE. Lets say this predictor is $X^*$.\n","2. Now, generate $m - 1$ models, each with $X^*$ and **one additional predictor**. Again, choose the model with the **lowest** MSE score.\n","3. Keep doing this until the MSE score doesn't improve **or** the $p$-value for some variable in the new model is higher than, say, $0.05$.\n","\n","Try both approaches and see if you reach the same final model."],"metadata":{"id":"YjNucUSbE1VW"}},{"cell_type":"code","source":["# load the Boston dataset\n","path = 'https://github.com/vladoxNCL/ml_course/raw/main/Boston.csv'\n","df = pd.read_csv(path)\n","df = df.iloc[:, 1:]\n","X = df.drop('medv', axis=1)\n","X = sm.add_constant(X)\n","y = df['medv']"],"metadata":{"id":"oRYKB03Ssc14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# do your modelling here"],"metadata":{"id":"c0y0znZvKVoI"},"execution_count":null,"outputs":[]}]}